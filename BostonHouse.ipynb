{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Boston housing dataset.\n",
    "\n",
    "The data wa s collected in 1978 and  originates from the UCI Machine Learning Repository. Each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset:\n",
    "\n",
    "16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed.\n",
    "1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed.\n",
    "The features 'RM', 'LSTAT', 'PTRATIO', and 'MEDV' are essential. The remaining non-relevant features have been excluded.\n",
    "The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation.\n",
    "\n",
    "Run the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import and print out dataset and credits.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "print(boston.keys())\n",
    "\n",
    "print(boston.data.shape)\n",
    "\n",
    "print(boston.feature_names)\n",
    "\n",
    "print(boston.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics and Plots.\n",
    "Convert to pandas, using the method pd.DataFrame() and pass boston.data and takes the first five data lines using head() last lines tail() and the numbers of lines len()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#complie data to give correct headings and place on readable table.\n",
    "bos = pd.DataFrame(boston.data)\n",
    "bos.head()\n",
    "\n",
    "\n",
    "bos.columns = boston.feature_names\n",
    "bos.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.DataFrame(boston.data)\n",
    "bos.tail()\n",
    "\n",
    "\n",
    "bos.columns = boston.feature_names\n",
    "bos.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length\n",
    "len(bos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data by dealing with any missing data, which Pandas automatically sets as NaN values. These can be identified by running df.isnull() , which returns a Boolean DataFrame. To get the number of NaN's per column, we can do df.isnull().sum() . \n",
    "\n",
    "For this dataset, we see there are no NaN's, which means we have no immediate work to do in cleaning the data and can move on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data.\n",
    "\n",
    "bos.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the analysis, the final thing we'll do before exploration is remove some of the columns. \n",
    "  Remove some columns by running the cell that contains the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ZN', 'NOX', 'RAD', 'PTRATIO', 'B']:\n",
    "     del bos[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes various properties including the mean, standard deviation, minimum, and maximum for each column. This table gives an idea of how everything is distributed.added { .T }to the output; to swap the rows and columns for asthetic purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos['PRICE'] = boston.target\n",
    "bos.head()\n",
    "\n",
    "bos.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Displaying the minimum, maximum, mean, median, and standard deviation of median value owner occupied homes in $1000's 'MEDV', which is stored in prices.\n",
    "Each calculation is stored in their respective variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import sklearn training and testing data splitter\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "#import visuals_md as vs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "\n",
    "    \n",
    "# Success\n",
    "print(\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))\n",
    "\n",
    "# Minimum price of the data\n",
    "minimum_price = prices.min()\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = prices.max()\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = prices.mean()\n",
    "\n",
    "# Median price of the data\n",
    "median_price = prices.median()\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = prices.std()\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"\\nStatistics for Boston housing dataset Prices:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(minimum_price)) \n",
    "print(\"Maximum price: ${:,.2f}\".format(maximum_price))\n",
    "print(\"Mean price: ${:,.2f}\".format(mean_price))\n",
    "print(\"Median price ${:,.2f}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(std_price))\n",
    "\n",
    "#data[[\"CHAS\", \"MEDV\"]].corr(method=\"pearson\")\n",
    "\n",
    "#data[[\"LSTAT\", \"MEDV\"]].corr(method=\"pearson\")\n",
    "\n",
    "#data[[\"PTRATIO\", \"MEDV\"]].corr(method=\"pearson\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create scatter plots to represent the data in pictorial form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection\n",
    "\n",
    "X = bos.drop('PRICE', axis = 1)\n",
    "Y = bos['PRICE']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = lm.predict(X_test)\n",
    "\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Prices: $Y_i$\")\n",
    "plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "plt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using three features from the Boston housing dataset: \n",
    "###### 'RM': is the average number of rooms among homes in the neighborhood.\n",
    "###### 'LSTAT': is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor).\n",
    "###### 'PTRATIO': is the ratio of students to teachers in primary and secondary schools in the neighborhood.\n",
    "For each data point (neighborhood):\n",
    "\n",
    "##### RM\n",
    "For a higher RM,  there is a higher MEDV.\n",
    "This is because more rooms would imply more space, thereby costing more, taking all other factors constant.\n",
    "\n",
    "##### LSTAT\n",
    "For a higher LSTAT,there is a lower MEDV.\n",
    "The area dominated by \"lower class\" citizens  may be deemed  unsafe compared to an area dominated by \"upper class\" citizens. Hence an area with more \"lower class\" citizens would have lower home demand, hence lower prices.\n",
    "\n",
    "##### PTRATIO\n",
    "For a higher LSTAT, one would expect to observe a lower MEDV.\n",
    "This is because there would be a higher student to teacher ratio resulting in less attention dedicated to each student that may impair their performance in school. Prices of houses around public schools where there is a high student to teacher ratio are generally lower than those with the opposite. Therefore expect a lower price given a high student-to-teacher ratio due to a lower demand for houses in such areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "\n",
    "plt.figure(figsize=(40, 10))\n",
    "\n",
    "# i: index\n",
    "for i, col in enumerate(features.columns):\n",
    "    # 3 plots here hence 1, 3\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    x = data[col]\n",
    "    y = prices\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse whether there is a significant difference in median house prices between those on the charles river and those not.\n",
    "\n",
    "room number, tax-full value tax property rate per $10000, dis distance from employment centers and indus-non retail business acres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['CHAS', 'RM', 'TAX','DIS','INDUS'] \n",
    "bos[cols].head()\n",
    "bos[cols].corr()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "ax = sns.heatmap(bos[cols].corr(),cmap=sns.cubehelix_palette(20, light=0.55, dark=0.85,))\n",
    "ax.xaxis.tick_top() # move labels to the top\n",
    "#plt.savefig(box_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=boston['data'], columns=boston['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "bos = pd.DataFrame(data=boston['data'], columns=boston['feature_names'])\n",
    "bos['MEDV'] = boston['target']\n",
    "y = bos['MEDV'].copy()\n",
    "del bos['MEDV']\n",
    "bos = pd.concat((y, bos), axis=1)\n",
    "for col in ['ZN', 'NOX', 'RAD', 'PTRATIO', 'B']:del bos[col]\n",
    "col = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV']\n",
    "bos[col].corr()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "ax = sns.heatmap(bos[cols].corr(),\n",
    "cmap=sns.cubehelix_palette(20, light=0.95, dark=0.15))\n",
    "ax.xaxis.tick_top() # move labels to the top\n",
    "#plt.savefig('../figures/chapter-1-boston-housing-corr.png',\n",
    "#bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "sns.regplot('RM', 'CHAS', bos, ax=ax[0],\n",
    "scatter_kws={'alpha': 0.4})\n",
    "sns.regplot('LSTAT', 'CHAS', bos, ax=ax[1],\n",
    "scatter_kws={'alpha': 0.4}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0] = sns.residplot('RM', 'MEDV', bos, ax=ax[0],\n",
    "scatter_kws={'alpha': 0.4})\n",
    "ax[0].set_ylabel('MDEV residuals $(y-\\hat{y})$')\n",
    "ax[1] = sns.residplot('LSTAT', 'MEDV', bos, ax=ax[1],\n",
    "scatter_kws={'alpha': 0.4})\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(bos, feature, target='MEDV'):\n",
    "# Get x, y to model\n",
    " y = bos[target].values\n",
    "x = bos[feature].values.reshape(-1,1)\n",
    "...\n",
    "...\n",
    "error = mean_squared_error(y, y_pred)\n",
    "print('mse = {:.2f}'.format(error))\n",
    "print() \n",
    "\n",
    "get_mse(bos, 'RM')\n",
    "get_mse(bos, 'CHAS') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'r2_score'\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score\n",
    "# Calculate the performance of this model\n",
    "score = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "print(\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'train_test_split'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.20, random_state=42)\n",
    "\n",
    "# Success\n",
    "print(\"Training and testing split was successful.\")\n",
    "\n",
    "\n",
    "# Produce learning curves for varying training set sizes and maximum depths\n",
    "#vs.ModelLearning(features, prices)\n",
    "\n",
    "\n",
    "# Produce model complexity validation curve\n",
    "#vs.ModelComplexity(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def fit_model(X, y):\n",
    "    \"\"\" \n",
    "    Performs grid search over the 'max_depth' parameter for a \n",
    "    decision tree regressor trained on the input data [X, y].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size=0.20, random_state=0)\n",
    "\n",
    "    # TODO: Create a decision tree regressor object\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {'max_depth': list(range(1, 11))}\n",
    "\n",
    "    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "\n",
    "    # TODO: Create the grid search cv object --> GridSearchCV()\n",
    "    # Make sure to include the right parameters in the object:\n",
    "    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n",
    "    grid = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=params,\n",
    "                        scoring =scoring_fnc,\n",
    "                        cv=cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "    \n",
    "    # Prepare the grid search table\n",
    "    depths = [d['max_depth'] for d in grid.cv_results_[\"params\"]]\n",
    "    scores = grid.cv_results_[\"mean_test_score\"]\n",
    "    df = pd.DataFrame({\"max_depth\": depths, \"mean_test_score\": scores}, \n",
    "                      columns=[\"max_depth\", \"mean_test_score\"])\n",
    "        \n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_, df\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Fit the training data to the model using grid search\n",
    "reg, grid_table = fit_model(X_train, y_train)\n",
    "\n",
    "# Display the grid search result table\n",
    "display(HTML(grid_table.to_html(index=False)))\n",
    "\n",
    "# Produce the value for 'max_depth'\n",
    "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a matrix for client data\n",
    "client_data = [[5, 17, 15], # Client 1\n",
    "               [4, 32, 22], # Client 2\n",
    "               [8, 3, 12]]  # Client 3\n",
    "\n",
    "# Show predictions\n",
    "for i, price in enumerate(reg.predict(client_data)):\n",
    "    print(\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))\n",
    "    \n",
    "    vs.PredictTrials(features, prices, fit_model, client_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline\n",
    "#sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "#sns.distplot(boston['MEDV'], bins=30)\n",
    "#plt.show()\n",
    "correlation_matrix = boston.corr().round(2)\n",
    "# annot = True to print the values inside the square\n",
    "sns.heatmap(data=correlation_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Calculate and show pairplot\n",
    "sns.pairplot(data, height=2.5)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show correlation matrix\n",
    "cm = np.corrcoef(data.values)\n",
    "sns.set(font_scale=1)\n",
    "hm = sns.heatmap(cm,\n",
    "                cbar=True,\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                annot_kws={'size': 15},\n",
    "                yticklabels=cols,\n",
    "                xticklabels=cols)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
